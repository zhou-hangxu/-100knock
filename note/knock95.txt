1.日本語のテキストを分割する
　SentencePieceはテキストをサブワードに分割するツールです。
　　モデルのファイル名のプレフィックスは。ボキャブラリーサイズは16000、カバーする文字の割合。

　Loadを使用して、訓練済みのモデルファイルを読み込んでいます

　下は90とほぼ同じ、さっきのprocessorを使って、テキストをサブワードに分割する。
　　結果はtrain.sub.ja…に保存する。encode_as_pieces()は、spを使って、テキストをトークンのリストに変換

2.英語の
　subword-nmtというツールを使って、

　まずは訓練データでBPEを学習して、結果はkyoto_en.codesというファイルに保存する
　　BPEの語彙サイズは16000です

　学習済みのBPEコードファイルを使って、入力ファイルのサブワード分割を行います

3.日本語（'ja'）から英語（'en'）への翻訳
　前とほぼ同じ
　まずは翻訳のために、データ前処理を行います、出力ファイルはdata95です

　fairseqライブラリを使用してニューラル機械翻訳モデルのトレーニングを行います
　エポック数を5

4.日本語の文を英語に翻訳するプログラムを実装です
　sedの後の内容は
　95.out'ファイルに結果が保存されます

5.95.out'ファイルの各行をSpaCyを使用してトークン化し、トークン化された結果を'95.out.spacy'ファイルに書き込みます

　指定されたシステムの出力ファイルと参照訳ファイルを使用して、BLEUスコアなどの評価指標が計算されます

6.ビームサイズが1から10まで変化しながら翻訳が行われ、各ビームサイズに対する翻訳結果がそれぞれのファイルに保存されます
　　各nに対して、spacy_tokenizeを使って、輸出したファイルは各行をSpaCyを使用してトークン化します。
　　BLEUスコアを95.n.scoreというファイルに保存します。
　
　
　https://torch.classcat.com/2021/03/27/pytorch-1-8-tutorials-intermediate-tensorboard/
　